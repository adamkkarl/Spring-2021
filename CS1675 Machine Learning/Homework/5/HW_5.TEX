\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx} 

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Homework 5}
\author{Adam Karl}

\begin{document}
\maketitle

\section{Problem 1}

b. 

\noindent
training data confusion matrix: 
\begin{table}[htb]
\begin{tabular}{ll}
0.4879 & 0.1651 \\
0.1410 & 0.2059
\end{tabular}
\end{table}

\noindent
training misclassification error = 0.3061

\noindent
testing data confusion matrix:
\begin{table}[htb]
\begin{tabular}{ll}
0.5284 & 0.0961 \\
0.1747 & .2009 
\end{tabular}
\end{table}

\noindent
testing misclassification error = 0.2707

\noindent
testing sensitivity = 0.7516

\noindent 
testing specificity = 0.6765




\noindent
c.
\begin{itemize}
    \item 10,000 epochs
    \begin{itemize}
        \item training misclassification error = 0.2839
        \item testing misclassification error = 0.2533
    \end{itemize}
    \item initial weights all 0.5
    \begin{itemize}
        \item training misclassification error = 0.3061
        \item testing misclassification error = 0.2707
    \end{itemize}
    \item initial weights all 0
    \begin{itemize}
        \item training misclassification error = 0.2542
        \item testing misclassification error = 0.2358
    \end{itemize}
    \item learning schedule = 1
    \begin{itemize}
        \item training misclassification error = 0.2913
        \item testing misclassification error = 0.3057
    \end{itemize}
    \item learning schedule = .1
    \begin{itemize}
        \item training misclassification error = 0.2820
        \item testing misclassification error = 0.3057
    \end{itemize}
\end{itemize}

\noindent
With my experimentation the minimum error was achieved with initial weights set to 0 but all other settings default. I was able to get a training misclassification error of 0.2542 and a testing misclassification error of 0.2358.


\subsection{Problem 2.1}

\begin{center}
    \includegraphics[scale=1]{0-1.png}
    \caption{class 0, variable 1}
\end{center}

\begin{center}
    \includegraphics[scale=1]{0-2.png}
    \caption{class 0, variable 2}
\end{center}

\begin{center}
    \includegraphics[scale=1]{0-3.png}
    \caption{class 0, variable 3}
\end{center}

\begin{center}
    \includegraphics[scale=1]{0-4.png}
    \caption{class 0, variable 4}
\end{center}

\begin{center}
    \includegraphics[scale=1]{0-5.png}
    \caption{class 0, variable 5}
\end{center}

\begin{center}
    \includegraphics[scale=1]{0-6.png}
    \caption{class 0, variable 6}
\end{center}

\begin{center}
    \includegraphics[scale=1]{0-7.png}
    \caption{class 0, variable 7}
\end{center}

\begin{center}
    \includegraphics[scale=1]{0-8.png}
    \caption{class 0, variable 8}
\end{center}



\begin{center}
    \includegraphics[scale=1]{1-1.png}
    \caption{class 1, variable 1}
\end{center}

\begin{center}
    \includegraphics[scale=1]{1-2.png}
    \caption{class 1, variable 2}
\end{center}

\begin{center}
    \includegraphics[scale=1]{1-3.png}
    \caption{class 1, variable 3}
\end{center}

\begin{center}
    \includegraphics[scale=1]{1-4.png}
    \caption{class 1, variable 4}
\end{center}

\begin{center}
    \includegraphics[scale=1]{1-5.png}
    \caption{class 1, variable 5}
\end{center}

\begin{center}
    \includegraphics[scale=1]{1-6.png}
    \caption{class 1, variable 6}
\end{center}

\begin{center}
    \includegraphics[scale=1]{1-7.png}
    \caption{class 1, variable 7}
\end{center}

\begin{center}
    \includegraphics[scale=1]{1-8.png}
    \caption{class 1, variable 8}
\end{center}


\noindent
b.
\begin{itemize}
    \item class 0
    \begin{itemize}
        \item variable 1: binomial
        \item variable 2: normal
        \item variable 3: normal
        \item variable 4: possibly loose normal, except for a lot of 0s which throw it off
        \item variable 5: gamma
        \item variable 6: normal
        \item variable 7: gamma
        \item variable 8: gamma
    \end{itemize}
    \item class 1
    \begin{itemize}
        \item variable 1: loose gamma
        \item variable 2: very loose normal
        \item variable 3: normal, except a fair amount of 0s which throw it off
        \item variable 4: possibly normal, except for a lot of 0s which throw it off
        \item variable 5: possibly normal, except for a lot of 0s which throw it off
        \item variable 6: normal
        \item variable 7: gamma
        \item variable 8: very loose normal
    \end{itemize}
\end{itemize}



\subsection{Problem 2.2}

\begin{itemize}
    \item CLASS 0 EXPFIT
    \begin{itemize}
        \item variable 1
        \begin{itemize}
            \item muhat = 3.2419
        \end{itemize}
        \item variable 2
        \begin{itemize}
            \item muhat = 109.6254
        \end{itemize}
        \item variable 3
        \begin{itemize}
            \item muhat = 67.5339
        \end{itemize}
        \item variable 4
        \begin{itemize}
            \item muhat = 19.7316
        \end{itemize}
        \item variable 5
        \begin{itemize}
            \item muhat = 67.7168
        \end{itemize}
        \item variable 6
        \begin{itemize}
            \item muhat = 30.3059
        \end{itemize}
        \item variable 7
        \begin{itemize}
            \item muhat = 0.4164
        \end{itemize}
        \item variable 8
        \begin{itemize}
            \item muhat = 31.1032
        \end{itemize}
    \end{itemize}
    
    
    \item CLASS 1 EXPFIT
    \begin{itemize}
        \item variable 1
        \begin{itemize}
            \item muhat = 4.7100
        \end{itemize}
        \item variable 2
        \begin{itemize}
            \item muhat = 141.3950
        \end{itemize}
        \item variable 3
        \begin{itemize}
            \item muhat = 70.1900
        \end{itemize}
        \item variable 4
        \begin{itemize}
            \item muhat = 22.9350
        \end{itemize}
        \item variable 5
        \begin{itemize}
            \item muhat = 103.7200
        \end{itemize}
        \item variable 6
        \begin{itemize}
            \item muhat = 35.2580
        \end{itemize}
        \item variable 7
        \begin{itemize}
            \item muhat = 0.5491
        \end{itemize}
        \item variable 8
        \begin{itemize}
            \item muhat = 37.1200
        \end{itemize}
    \end{itemize}
    
    \item CLASS 0 NORMFIT
    \begin{itemize}
        \item variable 1
        \begin{itemize}
            \item mu = 3.2419
            \item sigma = 2.9644
        \end{itemize}
        \item variable 2
        \begin{itemize}
            \item mu = 109.6254
            \item sigma = 26.2304
        \end{itemize}
        \item variable 3
        \begin{itemize}
            \item mu = 67.5339
            \item sigma = 18.6683
        \end{itemize}
        \item variable 4
        \begin{itemize}
            \item mu = 19.7316
            \item sigma = 14.5828
        \end{itemize}
        \item variable 5
        \begin{itemize}
            \item mu = 67.7168
            \item sigma = 91.6702
        \end{itemize}
        \item variable 6
        \begin{itemize}
            \item mu = 30.3059
            \item sigma = 7.7258
        \end{itemize}
        \item variable 7
        \begin{itemize}
            \item mu = 0.4164
            \item sigma = 0.2906
        \end{itemize}
        \item variable 8
        \begin{itemize}
            \item mu = 31.1032
            \item sigma = 11.3830
        \end{itemize}
    \end{itemize}
    
    
    \item CLASS 1 NORMFIT
    \begin{itemize}
        \item variable 1
        \begin{itemize}
            \item mu = 3.2419
            \item sigma = 2.9644
        \end{itemize}
        \item variable 2
        \begin{itemize}
            \item mu = 109.6254
            \item sigma = 26.2304
        \end{itemize}
        \item variable 3
        \begin{itemize}
            \item mu = 67.5339
            \item sigma = 18.6683
        \end{itemize}
        \item variable 4
        \begin{itemize}
            \item mu = 19.7316
            \item sigma = 14.5828
        \end{itemize}
        \item variable 5
        \begin{itemize}
            \item mu = 67.7168
            \item sigma = 91.6702
        \end{itemize}
        \item variable 6
        \begin{itemize}
            \item mu = 30.3059
            \item sigma = 7.7258
        \end{itemize}
        \item variable 7
        \begin{itemize}
            \item mu = 0.4164
            \item sigma = 0.2906
        \end{itemize}
        \item variable 8
        \begin{itemize}
            \item mu = 31.1032
            \item sigma = 11.3830
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{Problem 2.3}

a-b. 

\noindent
training data exp confusion matrix: 
\begin{table}[htb]
\begin{tabular}{ll}
0.5510 & 0.2430 \\
0.0779 & 0.1280
\end{tabular}
\end{table}

\noindent
training exp misclassification error = 0.3210

\noindent
training data norm confusion matrix: 
\begin{table}[htb]
\begin{tabular}{ll}
 0.5250 & 0.1466 \\
0.1039      & 0.2245
\end{tabular}
\end{table}

\noindent
training norm misclassification error = 0.2505

\noindent
testing exp confusion matrix:
\begin{table}[htb]
\begin{tabular}{ll}
0.6201 & 0.2009 \\
0.0830 & 0.0961 
\end{tabular}
\end{table}

\noindent
testing exp misclassification error = 0.2838

\noindent
testing exp sensitivity = 0.8820

\noindent 
testing exp specificity = 0.3235



\noindent
testing norm confusion matrix:


\begin{tabular}{ll}
0.5677 & 0.1092 \\
0.1354 & 0.1878
\end{tabular}


\noindent
testing norm misclassification error = 0.2445

\noindent
testing norm sensitivity = 0.8075

\noindent 
testing norm specificity = 0.6324


\noindent
c. 

\noindent
For the training data, the exponential prediction slightly underperformed the logistic regression model with 0.3210 misclassification, up from 0.3061. The normal distribution prediction overperformed the logistic regression model with 0.2505 misclassification, down from 0.3061.


For the testing data, the exponential prediction marginally underperformed the logistic regression model with 0.2838 misclassification, up from 0.2707. The normal distribution prediction also overperformed the logistic regression model with 0.2445 misclassification, down from 0.2707.


The best model in this assignment is the normal distribution model, with the lowest misclassification error of 0.2505 for training and 0.2445 for testing. Thise model also had a sensitivity of 0.8075 and a specificity of 0.6324.

\end{document}
