\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx} 

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Homework 4}
\author{Adam Karl}

\begin{document}
\maketitle

\section{Exploratory Data Analysis}

\noindent
a. There is only one binary attribute, named CHAS. It is 1 if the tract touches the Charles River, or 0 otherwise.

\noindent 
b. 

\begin{itemize}
    \item CRIM: -0.3883
    \item ZN: 0.3604
    \item INDUS: -0.4837
    \item CHAS: 0.1753
    \item NOX: -0.4273
    \item RM: 0.6954
    \item AGE: -0.3770
    \item DIS: 0.2499
    \item RAD: -0.3816
    \item TAX: -0.4685
    \item PTRATIO: -0.5078
    \item B: 0.3335
    \item LSTAT: -0.7377
\end{itemize}

\noindent
The highest positive correlation is between RM (average number of homes in a dwelling) and average home value with 0.6954. The highest negative correlation is between LSTAT (percentage of lower status population) and home value with -0.7377.


\noindent
c. In my opinion, the scatter plot of XXX vsLSTAT looks the most linear, and shows an obvious negative correlation. However, there does appear to be a slight curve to the trend.

\begin{center}
    \includegraphics[scale=1]{1c-1.png}
    \caption{LSTAT vs MEDV}
\end{center}

\noindent
Depending on interpretation, I'm between DIS vs MEDV and CHAS vs MEDV for the most nonlinear graph. While DIS vs MEDV has the vaguest of positive correlations, practically no information or correlation can be gleaned from CHAS (a binary attribute) vs MEDV.

\begin{center}
    \includegraphics[scale=1]{1c-2.png}
    \caption{DIS vs MEDV}
\end{center}

\begin{center}
    \includegraphics[scale=1]{1c-3.png}
    \caption{CHAS vs MEDV}
\end{center}

\noindent 
d. The highest correlation is between RAD (index of accessibility to radial highways) and TAX (full-value property-tax rate per \$10,000) with a correlation of 0.9102.

\section{Linear Regression}
d. Applying the training set model to both the training and testing data, I got a mean squared error of 24.4759 for the training data and 24.2922 for the testing data. It's a bit odd that the model fits the test data better than the data the model is based on, but part of this is likely because the test data has a smaller sample size.


\section{Online (stochastic) gradient descent}
a. Applying the online model after 1000 iterations to both the training and testing data, I got a mean squared error of 312.0710 for the training data and 251.5408 for the testing data. The model still fits the test data better than the training data, but the errors are much higher for both compared to the linear regression offline model.

\noindent
b. Feeding in the non-normalized data, the model seems to break since my function returns NaN for all of the final values. It would appear this method only works with normalized data.

\noindent
c. 
\begin{center}
    \includegraphics[scale=1]{3c.png}
    \caption{CHAS vs MEDV}
\end{center}

\noindent
d. I first tried adding additional steps. Adding steps keeps decreasing the error, but at a decreasing rate of return.

\begin{center}
    \includegraphics[scale=1]{3d-1.png}
    \caption{10,000 steps}
\end{center}

\noindent
Then I created a set learning rate. This does allow the error to come down significantly, at the cost of a more unstable error after many iterations.

\begin{center}
    \includegraphics[scale=1]{3d-2.png}
    \caption{rate = 0.05}
\end{center}

\noindent 
Trying 2/sqrt(n) for the learning rate, on the other hand, makes the error blow up out of control.

\begin{center}
    \includegraphics[scale=1]{3d-3.png}
    \caption{rate = 2/sqrt(n)}
\end{center}

\end{document}